{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3990913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.2.4)\n",
      "Requirement already satisfied: transformers in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (4.51.2)\n",
      "Requirement already satisfied: accelerate in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: datasets in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: tokenizers in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (0.21.1)\n",
      "Requirement already satisfied: seqeval in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: evaluate in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pandas numpy transformers accelerate datasets tokenizers seqeval evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad36e4-2065-4ab3-a522-4eef0530cb98",
   "metadata": {},
   "source": [
    "## Token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca459c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcanodeb/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datasets \n",
    "import numpy as np \n",
    "from transformers import BertTokenizerFast, RobertaTokenizerFast, AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification \n",
    "from transformers import AutoModelForTokenClassification \n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset, DatasetDict, Features, Value, Sequence, ClassLabel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d555d5-2565-4fb6-9837-3722bf1a3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "dataset_to_use = \"biored\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780f833b-1f41-4d28-b1b8-4b02688db304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': 'Late-onset', 'sentence_id': 0, 'labels': 'O'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(dtype='string', id=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"./datasets/preprocessed_NER/\"+dataset_to_use+\"/train.csv\", \"validation\": \"./datasets/preprocessed_NER/\"+dataset_to_use+\"/dev.csv\", \"test\": \"./datasets/preprocessed_NER/\"+dataset_to_use+\"/test.csv\"}\n",
    "raw_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "print(raw_dataset[\"train\"][0])\n",
    "raw_dataset[\"train\"].features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49242446-1d70-42d2-9d90-e54106770480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|███████████████████████████████████████████████████████████████████| 4342/4342 [00:00<00:00, 14587.36 examples/s]\n",
      "Casting the dataset: 100%|███████████████████████████████████████████████████████████████████| 1127/1127 [00:00<00:00, 34354.80 examples/s]\n",
      "Casting the dataset: 100%|███████████████████████████████████████████████████████████████████| 1096/1096 [00:00<00:00, 34930.22 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_id': 0, 'words': ['Late-onset', 'metachromatic', 'leukodystrophy', ':', 'molecular', 'pathology', 'in', 'two', 'siblings', '.'], 'labels': [12, 2, 8, 12, 12, 12, 12, 12, 12, 12]}\n",
      "List of labels: ['B-CellLine', 'B-ChemicalEntity', 'B-DiseaseOrPhenotypicFeature', 'B-GeneOrGeneProduct', 'B-OrganismTaxon', 'B-SequenceVariant', 'I-CellLine', 'I-ChemicalEntity', 'I-DiseaseOrPhenotypicFeature', 'I-GeneOrGeneProduct', 'I-OrganismTaxon', 'I-SequenceVariant', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_datasets = {}\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    dataset = raw_dataset[split]\n",
    "    grouped = defaultdict(lambda: {\"words\": [], \"labels\": []})\n",
    "\n",
    "    for example in dataset:\n",
    "        sid = example[\"sentence_id\"]\n",
    "        grouped[sid][\"words\"].append(example[\"words\"])\n",
    "        grouped[sid][\"labels\"].append(example[\"labels\"])\n",
    "\n",
    "    grouped_list = []\n",
    "    for sid, data in grouped.items():\n",
    "        grouped_list.append({\n",
    "            \"sentence_id\": sid,\n",
    "            \"words\": data[\"words\"],\n",
    "            \"labels\": data[\"labels\"]\n",
    "        })\n",
    "    grouped_datasets[split] = Dataset.from_list(grouped_list)\n",
    "\n",
    "all_labels = set()\n",
    "for example in grouped_datasets[\"train\"]:\n",
    "    all_labels.update(example[\"labels\"])\n",
    "label_list = sorted(list(all_labels))\n",
    "\n",
    "label_feature = ClassLabel(names=label_list)\n",
    "\n",
    "features = Features({\n",
    "    \"sentence_id\": Value(\"int32\"),\n",
    "    \"words\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(label_feature),\n",
    "})\n",
    "\n",
    "final_datasets = DatasetDict()\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    grouped_datasets[split] = grouped_datasets[split].cast(features)\n",
    "\n",
    "final_datasets = DatasetDict(grouped_datasets)\n",
    "\n",
    "print(final_datasets[\"train\"][0])\n",
    "print(\"List of labels:\", final_datasets[\"train\"].features[\"labels\"].feature.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895fa0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (4342, 3), 'validation': (1127, 3), 'test': (1096, 3)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de4616e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_id': 0,\n",
       " 'words': ['Late-onset',\n",
       "  'metachromatic',\n",
       "  'leukodystrophy',\n",
       "  ':',\n",
       "  'molecular',\n",
       "  'pathology',\n",
       "  'in',\n",
       "  'two',\n",
       "  'siblings',\n",
       "  '.'],\n",
       " 'labels': [12, 2, 8, 12, 12, 12, 12, 12, 12, 12]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925566eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['B-CellLine', 'B-ChemicalEntity', 'B-DiseaseOrPhenotypicFeature', 'B-GeneOrGeneProduct', 'B-OrganismTaxon', 'B-SequenceVariant', 'I-CellLine', 'I-ChemicalEntity', 'I-DiseaseOrPhenotypicFeature', 'I-GeneOrGeneProduct', 'I-OrganismTaxon', 'I-SequenceVariant', 'O'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets[\"train\"].features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ca23fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_to_use) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3318b88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 3, 4, 5, 5, 6, 7, 8, 9, None]\n"
     ]
    }
   ],
   "source": [
    "example_text = final_datasets['train'][0]\n",
    "tokenized_input = tokenizer(example_text[\"words\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "word_ids = tokenized_input.word_ids()\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "079ccd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'late',\n",
       " '-',\n",
       " 'onset',\n",
       " 'meta',\n",
       " '##ch',\n",
       " '##romatic',\n",
       " 'le',\n",
       " '##uk',\n",
       " '##ody',\n",
       " '##stro',\n",
       " '##phy',\n",
       " ':',\n",
       " 'molecular',\n",
       " 'path',\n",
       " '##ology',\n",
       " 'in',\n",
       " 'two',\n",
       " 'siblings',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0cc280c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 21)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_text['labels']), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a2567c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    texts = [[str(token) for token in sent] for sent in examples[\"words\"]]\n",
    "    \n",
    "    tokenized_inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    labels_all = []\n",
    "    for i, word_labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(word_labels[word_idx])\n",
    "            else:\n",
    "                # We decide whether sub-tokens inherit the label or are ignored.\n",
    "                # Ignore -> -100 ; if not -> word_labels[word_idx].\n",
    "                label_ids.append(word_labels[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels_all.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels_all\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c84ac8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_id': [4],\n",
       " 'words': [['A',\n",
       "   'comparison',\n",
       "   'of',\n",
       "   'genotypes',\n",
       "   ',',\n",
       "   'ARSA',\n",
       "   'activities',\n",
       "   ',',\n",
       "   'and',\n",
       "   'clinical',\n",
       "   'data',\n",
       "   'on',\n",
       "   '4',\n",
       "   'individuals',\n",
       "   'carrying',\n",
       "   'the',\n",
       "   'allele',\n",
       "   'of',\n",
       "   '81',\n",
       "   'patients',\n",
       "   'with',\n",
       "   'MLD',\n",
       "   'examined',\n",
       "   ',',\n",
       "   'further',\n",
       "   'validates',\n",
       "   'the',\n",
       "   'concept',\n",
       "   'that',\n",
       "   'different',\n",
       "   'degrees',\n",
       "   'of',\n",
       "   'residual',\n",
       "   'ARSA',\n",
       "   'activity',\n",
       "   'are',\n",
       "   'the',\n",
       "   'basis',\n",
       "   'of',\n",
       "   'phenotypical',\n",
       "   'variation',\n",
       "   'in',\n",
       "   'MLD',\n",
       "   '..']],\n",
       " 'labels': [[12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   3,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   4,\n",
       "   12,\n",
       "   2,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   3,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   2,\n",
       "   12]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets['train'][4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e0e19e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 170, 7577, 1104, 176, 26601, 15177, 1116, 117, 170, 24129, 2619, 117, 1105, 7300, 2233, 1113, 125, 2833, 4004, 1103, 1155, 11194, 1104, 5615, 4420, 1114, 182, 5253, 8600, 117, 1748, 9221, 5430, 1103, 3400, 1115, 1472, 4842, 1104, 25399, 170, 24129, 3246, 1132, 1103, 3142, 1104, 185, 10436, 27202, 1348, 8516, 1107, 182, 5253, 119, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 12, 12, 12, 12, 12, 12, 12, 12, 3, 3, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 4, 12, 2, 2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 3, 3, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 2, 2, 12, 12, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]]}\n"
     ]
    }
   ],
   "source": [
    "q = tokenize_and_align_labels(final_datasets['train'][4:5]) \n",
    "print(q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbfd5ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "a_______________________________________ 12\n",
      "comparison______________________________ 12\n",
      "of______________________________________ 12\n",
      "g_______________________________________ 12\n",
      "##eno___________________________________ 12\n",
      "##type__________________________________ 12\n",
      "##s_____________________________________ 12\n",
      ",_______________________________________ 12\n",
      "a_______________________________________ 3\n",
      "##rsa___________________________________ 3\n",
      "activities______________________________ 12\n",
      ",_______________________________________ 12\n",
      "and_____________________________________ 12\n",
      "clinical________________________________ 12\n",
      "data____________________________________ 12\n",
      "on______________________________________ 12\n",
      "4_______________________________________ 12\n",
      "individuals_____________________________ 12\n",
      "carrying________________________________ 12\n",
      "the_____________________________________ 12\n",
      "all_____________________________________ 12\n",
      "##ele___________________________________ 12\n",
      "of______________________________________ 12\n",
      "81______________________________________ 12\n",
      "patients________________________________ 4\n",
      "with____________________________________ 12\n",
      "m_______________________________________ 2\n",
      "##ld____________________________________ 2\n",
      "examined________________________________ 12\n",
      ",_______________________________________ 12\n",
      "further_________________________________ 12\n",
      "valid___________________________________ 12\n",
      "##ates__________________________________ 12\n",
      "the_____________________________________ 12\n",
      "concept_________________________________ 12\n",
      "that____________________________________ 12\n",
      "different_______________________________ 12\n",
      "degrees_________________________________ 12\n",
      "of______________________________________ 12\n",
      "residual________________________________ 12\n",
      "a_______________________________________ 3\n",
      "##rsa___________________________________ 3\n",
      "activity________________________________ 12\n",
      "are_____________________________________ 12\n",
      "the_____________________________________ 12\n",
      "basis___________________________________ 12\n",
      "of______________________________________ 12\n",
      "p_______________________________________ 12\n",
      "##hen___________________________________ 12\n",
      "##otypic________________________________ 12\n",
      "##al____________________________________ 12\n",
      "variation_______________________________ 12\n",
      "in______________________________________ 12\n",
      "m_______________________________________ 2\n",
      "##ld____________________________________ 2\n",
      "._______________________________________ 12\n",
      "._______________________________________ 12\n",
      "[SEP]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]): \n",
    "    print(f\"{token:_<40} {label}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54b24a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████| 4342/4342 [00:00<00:00, 8683.80 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████| 1127/1127 [00:00<00:00, 11165.13 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████| 1096/1096 [00:00<00:00, 11323.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = final_datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bcd7e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_id': 0, 'words': ['Late-onset', 'metachromatic', 'leukodystrophy', ':', 'molecular', 'pathology', 'in', 'two', 'siblings', '.'], 'labels': [-100, 12, 12, 12, 2, 2, 2, 8, 8, 8, 8, 8, 12, 12, 12, 12, 12, 12, 12, 12, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], 'input_ids': [101, 1523, 118, 15415, 27154, 1732, 16341, 5837, 7563, 22320, 21216, 22192, 131, 9546, 3507, 4807, 1107, 1160, 9302, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(str(tokenized_datasets['train'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4849200f-ca4c-4e31-83ab-66c842da82e8",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48f3d084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(label_list) \n",
    "model = AutoModelForTokenClassification.from_pretrained(model_to_use, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33281cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer \n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output-ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    #warmup_ratio=0.1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "166fb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f35e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef327ef2-3520-455d-b104-719f4c8ca892",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59bed31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = final_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f774a8a-2192-42a6-a356-67054fd9baca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-CellLine',\n",
       " 'B-ChemicalEntity',\n",
       " 'B-DiseaseOrPhenotypicFeature',\n",
       " 'B-GeneOrGeneProduct',\n",
       " 'B-OrganismTaxon',\n",
       " 'B-SequenceVariant',\n",
       " 'I-CellLine',\n",
       " 'I-ChemicalEntity',\n",
       " 'I-DiseaseOrPhenotypicFeature',\n",
       " 'I-GeneOrGeneProduct',\n",
       " 'I-OrganismTaxon',\n",
       " 'I-SequenceVariant',\n",
       " 'O']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = final_datasets[\"train\"].features[\"labels\"].feature.names \n",
    "\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3528bed-47d9-4315-be33-33be579452c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "2\n",
      "8\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "for i in example[\"labels\"]:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d4414f4-77d7-4555-a997-dbe891f06ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-DiseaseOrPhenotypicFeature',\n",
       " 'I-DiseaseOrPhenotypicFeature',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[\"labels\"]] \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fb97c73-4cef-4a92-9cb0-8fdad8f2a360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DiseaseOrPhenotypicFeature': {'precision': np.float64(1.0),\n",
       "  'recall': np.float64(1.0),\n",
       "  'f1': np.float64(1.0),\n",
       "  'number': np.int64(1)},\n",
       " 'overall_precision': np.float64(1.0),\n",
       " 'overall_recall': np.float64(1.0),\n",
       " 'overall_f1': np.float64(1.0),\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=[labels], references=[labels]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc44bd-3094-4933-8cdc-20165cd5368b",
   "metadata": {},
   "source": [
    "### Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28722ba1-00f8-434a-84c5-3f77b339b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, labels = eval_preds \n",
    "    pred_ids = np.argmax(pred_logits, axis=2) \n",
    "\n",
    "    predictions = [ \n",
    "        [label_list[p] for (p, l) in zip(pred, label) if l != -100] \n",
    "        for pred, label in zip(pred_ids, labels)\n",
    "    ] \n",
    "\n",
    "    true_labels = [ \n",
    "        [label_list[l] for (p, l) in zip(pred, label) if l != -100] \n",
    "        for pred, label in zip(pred_ids, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=predictions, references=true_labels)\n",
    "\n",
    "    return { \n",
    "          \"precision\": results[\"overall_precision\"], \n",
    "          \"recall\": results[\"overall_recall\"], \n",
    "          \"f1\": results[\"overall_f1\"], \n",
    "          \"accuracy\": results[\"overall_accuracy\"], \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56886c-1fb7-4094-bb74-8afb9725891b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc2eef4f-b9f9-4196-864d-15bd6862dcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3004228/2660706628.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer( \n",
    "   model, \n",
    "   args, \n",
    "   train_dataset=tokenized_datasets[\"train\"], \n",
    "   eval_dataset=tokenized_datasets[\"validation\"], \n",
    "   data_collator=data_collator, \n",
    "   tokenizer=tokenizer, \n",
    "   compute_metrics=compute_metrics \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f5034e5-9beb-4457-b81f-de6973bf155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "train_batch_size = args.per_device_train_batch_size\n",
    "total_steps = (len(tokenized_datasets[\"train\"]) // train_batch_size) * args.num_train_epochs\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainer.model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "trainer.optimizer = optimizer\n",
    "trainer.lr_scheduler = scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49748c83-1540-481a-b150-576ca852f257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5430' max='5430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5430/5430 02:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>0.246652</td>\n",
       "      <td>0.842560</td>\n",
       "      <td>0.874394</td>\n",
       "      <td>0.858182</td>\n",
       "      <td>0.934655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.255503</td>\n",
       "      <td>0.865478</td>\n",
       "      <td>0.879334</td>\n",
       "      <td>0.872351</td>\n",
       "      <td>0.937596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.093400</td>\n",
       "      <td>0.273095</td>\n",
       "      <td>0.889857</td>\n",
       "      <td>0.878053</td>\n",
       "      <td>0.883916</td>\n",
       "      <td>0.943389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.317662</td>\n",
       "      <td>0.883496</td>\n",
       "      <td>0.894246</td>\n",
       "      <td>0.888838</td>\n",
       "      <td>0.945817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.325094</td>\n",
       "      <td>0.882635</td>\n",
       "      <td>0.892325</td>\n",
       "      <td>0.887453</td>\n",
       "      <td>0.945282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5430, training_loss=0.15150197805291382, metrics={'train_runtime': 126.7621, 'train_samples_per_second': 171.266, 'train_steps_per_second': 42.836, 'total_flos': 1418329186460160.0, 'train_loss': 0.15150197805291382, 'epoch': 5.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3876e48f-0082-44f8-a465-854bd2cdd8b8",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02b96ce3-2b7a-4331-b590-10e4407d318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4366df8-830b-4a6d-bfc7-cc0e5a2cd6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/vocab.txt',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d69f826-1b69-43e6-8896-b9db24aacdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    str(i): label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: str(i) for i,label in enumerate(label_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "672f51b1-1a9b-403f-8a49-1fc5ece3a7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'B-CellLine',\n",
       " '1': 'B-ChemicalEntity',\n",
       " '2': 'B-DiseaseOrPhenotypicFeature',\n",
       " '3': 'B-GeneOrGeneProduct',\n",
       " '4': 'B-OrganismTaxon',\n",
       " '5': 'B-SequenceVariant',\n",
       " '6': 'I-CellLine',\n",
       " '7': 'I-ChemicalEntity',\n",
       " '8': 'I-DiseaseOrPhenotypicFeature',\n",
       " '9': 'I-GeneOrGeneProduct',\n",
       " '10': 'I-OrganismTaxon',\n",
       " '11': 'I-SequenceVariant',\n",
       " '12': 'O'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "288bc1c9-34e5-4967-8681-882a9a73f15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-CellLine': '0',\n",
       " 'B-ChemicalEntity': '1',\n",
       " 'B-DiseaseOrPhenotypicFeature': '2',\n",
       " 'B-GeneOrGeneProduct': '3',\n",
       " 'B-OrganismTaxon': '4',\n",
       " 'B-SequenceVariant': '5',\n",
       " 'I-CellLine': '6',\n",
       " 'I-ChemicalEntity': '7',\n",
       " 'I-DiseaseOrPhenotypicFeature': '8',\n",
       " 'I-GeneOrGeneProduct': '9',\n",
       " 'I-OrganismTaxon': '10',\n",
       " 'I-SequenceVariant': '11',\n",
       " 'O': '12'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9493db-0817-4991-8791-5f4e004e625c",
   "metadata": {},
   "source": [
    "## Loading model and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d71b6463-a750-409c-b7b0-5ebddbe059e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "603493d7-39fe-4965-9bef-3817b802bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"ner_model/config.json\"))\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6951ad1-128a-4fe2-8b84-f26ab7417ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b041b97b-2dda-4d62-bb55-6b14ea824f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████| 1096/1096 [00:00<00:00, 10680.96 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='274' max='274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [274/274 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2692228853702545, 'eval_precision': 0.8720277851990382, 'eval_recall': 0.8941649164459866, 'eval_f1': 0.8829576194770062, 'eval_accuracy': 0.9487617037519366, 'eval_runtime': 2.0217, 'eval_samples_per_second': 542.129, 'eval_steps_per_second': 135.532, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"ner_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
    "tokenized_test = final_datasets[\"test\"].map(tokenize_and_align_labels, batched=True)\n",
    "results = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf2504-7d3d-4fdc-921f-a93e0b936015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351932f7-2ec1-446a-9f67-d4aba0f40ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d7337-687d-41b2-96a3-9c3d35249f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
