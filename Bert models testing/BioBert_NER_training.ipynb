{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3990913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.2.4)\n",
      "Requirement already satisfied: transformers in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (4.51.1)\n",
      "Requirement already satisfied: accelerate in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: datasets in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: tokenizers in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (0.21.1)\n",
      "Requirement already satisfied: seqeval in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: evaluate in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pandas numpy transformers accelerate datasets tokenizers seqeval evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad36e4-2065-4ab3-a522-4eef0530cb98",
   "metadata": {},
   "source": [
    "## Token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca459c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcanodeb/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datasets \n",
    "import numpy as np \n",
    "from transformers import BertTokenizerFast \n",
    "from transformers import DataCollatorForTokenClassification \n",
    "from transformers import AutoModelForTokenClassification \n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset, DatasetDict, Features, Value, Sequence, ClassLabel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780f833b-1f41-4d28-b1b8-4b02688db304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': 'Glucose', 'sentence_id': 0, 'labels': 'O'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(dtype='string', id=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"./datasets/preprocessed_NER/ncbi-disease/train.csv\", \"validation\": \"./datasets/preprocessed_NER/ncbi-disease/dev.csv\", \"test\": \"./datasets/preprocessed_NER/ncbi-disease/test.csv\"}\n",
    "raw_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "print(raw_dataset[\"train\"][0])\n",
    "raw_dataset[\"train\"].features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49242446-1d70-42d2-9d90-e54106770480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|████████████████████████████| 5729/5729 [00:01<00:00, 5588.20 examples/s]\n",
      "Casting the dataset: 100%|█████████████████████████████| 947/947 [00:00<00:00, 28981.53 examples/s]\n",
      "Casting the dataset: 100%|█████████████████████████████| 978/978 [00:00<00:00, 29296.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_id': 0, 'words': ['Glucose', '6-phosphate', 'dehydrogenase', 'variants', ':', 'Gd', '(', '+', ')', 'Alexandra', 'associated', 'with', 'neonatal', 'jaundice', 'and', 'Gd', '(', '-', ')', 'Camperdown', 'in', 'a', 'young', 'man', 'with', 'lamellar', 'cataracts', '.'], 'labels': [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]}\n",
      "List of labels: ['B-CompositeMention', 'B-DiseaseClass', 'B-Modifier', 'B-SpecificDisease', 'I-CompositeMention', 'I-DiseaseClass', 'I-Modifier', 'I-SpecificDisease', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_datasets = {}\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    dataset = raw_dataset[split]\n",
    "    grouped = defaultdict(lambda: {\"words\": [], \"labels\": []})\n",
    "\n",
    "    for example in dataset:\n",
    "        sid = example[\"sentence_id\"]\n",
    "        grouped[sid][\"words\"].append(example[\"words\"])\n",
    "        grouped[sid][\"labels\"].append(example[\"labels\"])\n",
    "\n",
    "    grouped_list = []\n",
    "    for sid, data in grouped.items():\n",
    "        grouped_list.append({\n",
    "            \"sentence_id\": sid,\n",
    "            \"words\": data[\"words\"],\n",
    "            \"labels\": data[\"labels\"]\n",
    "        })\n",
    "    grouped_datasets[split] = Dataset.from_list(grouped_list)\n",
    "\n",
    "all_labels = set()\n",
    "for example in grouped_datasets[\"train\"]:\n",
    "    all_labels.update(example[\"labels\"])\n",
    "label_list = sorted(list(all_labels))\n",
    "\n",
    "label_feature = ClassLabel(names=label_list)\n",
    "\n",
    "features = Features({\n",
    "    \"sentence_id\": Value(\"int32\"),\n",
    "    \"words\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(label_feature),\n",
    "})\n",
    "\n",
    "final_datasets = DatasetDict()\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    grouped_datasets[split] = grouped_datasets[split].cast(features)\n",
    "\n",
    "final_datasets = DatasetDict(grouped_datasets)\n",
    "\n",
    "print(final_datasets[\"train\"][0])\n",
    "print(\"List of labels:\", final_datasets[\"train\"].features[\"labels\"].feature.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895fa0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (5729, 3), 'validation': (947, 3), 'test': (978, 3)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de4616e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_id': 0,\n",
       " 'words': ['Glucose',\n",
       "  '6-phosphate',\n",
       "  'dehydrogenase',\n",
       "  'variants',\n",
       "  ':',\n",
       "  'Gd',\n",
       "  '(',\n",
       "  '+',\n",
       "  ')',\n",
       "  'Alexandra',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'neonatal',\n",
       "  'jaundice',\n",
       "  'and',\n",
       "  'Gd',\n",
       "  '(',\n",
       "  '-',\n",
       "  ')',\n",
       "  'Camperdown',\n",
       "  'in',\n",
       "  'a',\n",
       "  'young',\n",
       "  'man',\n",
       "  'with',\n",
       "  'lamellar',\n",
       "  'cataracts',\n",
       "  '.'],\n",
       " 'labels': [8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925566eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['B-CompositeMention', 'B-DiseaseClass', 'B-Modifier', 'B-SpecificDisease', 'I-CompositeMention', 'I-DiseaseClass', 'I-Modifier', 'I-SpecificDisease', 'O'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets[\"train\"].features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ca23fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3318b88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 1, 1, 2, 2, 2, 2, 2, 3, 4, 5, 5, 6, 7, 8, 9, 9, 9, 10, 11, 12, 12, 12, 13, 13, 13, 13, 14, 15, 15, 16, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 25, 25, 26, 26, 26, 27, None]\n"
     ]
    }
   ],
   "source": [
    "example_text = final_datasets['train'][0]\n",
    "tokenized_input = tokenizer(example_text[\"words\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "word_ids = tokenized_input.word_ids()\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079ccd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'glucose',\n",
       " '6',\n",
       " '-',\n",
       " 'phosphate',\n",
       " 'de',\n",
       " '##hy',\n",
       " '##dr',\n",
       " '##ogen',\n",
       " '##ase',\n",
       " 'variants',\n",
       " ':',\n",
       " 'g',\n",
       " '##d',\n",
       " '(',\n",
       " '+',\n",
       " ')',\n",
       " 'ale',\n",
       " '##xa',\n",
       " '##ndra',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'neon',\n",
       " '##ata',\n",
       " '##l',\n",
       " 'j',\n",
       " '##au',\n",
       " '##ndi',\n",
       " '##ce',\n",
       " 'and',\n",
       " 'g',\n",
       " '##d',\n",
       " '(',\n",
       " '-',\n",
       " ')',\n",
       " 'camp',\n",
       " '##erd',\n",
       " '##own',\n",
       " 'in',\n",
       " 'a',\n",
       " 'young',\n",
       " 'man',\n",
       " 'with',\n",
       " 'lame',\n",
       " '##llar',\n",
       " 'cat',\n",
       " '##ara',\n",
       " '##cts',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0cc280c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_text['labels']), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a2567c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    texts = [[str(token) for token in sent] for sent in examples[\"words\"]]\n",
    "    label_all_tokens=False\n",
    "    tokenized_inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    labels_all = []\n",
    "    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"words\"]))):\n",
    "        word_labels = examples[\"labels\"][i]\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(word_labels[word_idx])\n",
    "            else:\n",
    "                label_ids.append(word_labels[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels_all.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels_all\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c84ac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_id': [4], 'words': [['Although', 'this', 'association', 'may', 'be', 'coincidental', ',', 'it', 'prompts', 'further', 'attention', 'to', 'the', 'possibility', 'that', 'under', 'certain', 'circumstances', 'G6PD', 'deficiency', 'may', 'favor', 'cataract', 'formation', '.']], 'labels': [[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 7, 8, 8, 2, 8, 8]]}\n"
     ]
    }
   ],
   "source": [
    "print(str(final_datasets['train'][4:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e0e19e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1780,  1142,  3852,  1336,  1129, 21439, 21739,   117,  1122,\n",
      "          5250, 18378,  1116,  1748,  2209,  1106,  1103,  5417,  1115,  1223,\n",
      "          2218,  5607,   176,  1545,  1643,  1181, 21344,  1336,  5010,  5855,\n",
      "          4626,  5822,  3855,   119,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': [[-100, 8, 8, 8, 8, 8, 8, -100, 8, 8, 8, -100, -100, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, -100, -100, -100, 7, 8, 8, 2, -100, -100, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]]}\n"
     ]
    }
   ],
   "source": [
    "q = tokenize_and_align_labels(final_datasets['train'][4:5]) \n",
    "print(q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbfd5ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "although________________________________ 8\n",
      "this____________________________________ 8\n",
      "association_____________________________ 8\n",
      "may_____________________________________ 8\n",
      "be______________________________________ 8\n",
      "coincide________________________________ 8\n",
      "##ntal__________________________________ -100\n",
      ",_______________________________________ 8\n",
      "it______________________________________ 8\n",
      "pro_____________________________________ 8\n",
      "##mpt___________________________________ -100\n",
      "##s_____________________________________ -100\n",
      "further_________________________________ 8\n",
      "attention_______________________________ 8\n",
      "to______________________________________ 8\n",
      "the_____________________________________ 8\n",
      "possibility_____________________________ 8\n",
      "that____________________________________ 8\n",
      "under___________________________________ 8\n",
      "certain_________________________________ 8\n",
      "circumstances___________________________ 8\n",
      "g_______________________________________ 3\n",
      "##6_____________________________________ -100\n",
      "##p_____________________________________ -100\n",
      "##d_____________________________________ -100\n",
      "deficiency______________________________ 7\n",
      "may_____________________________________ 8\n",
      "favor___________________________________ 8\n",
      "cat_____________________________________ 2\n",
      "##ara___________________________________ -100\n",
      "##ct____________________________________ -100\n",
      "formation_______________________________ 8\n",
      "._______________________________________ 8\n",
      "[SEP]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]): \n",
    "    print(f\"{token:_<40} {label}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f54b24a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████| 5729/5729 [00:01<00:00, 4367.26 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████| 947/947 [00:00<00:00, 4662.90 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████| 978/978 [00:00<00:00, 4811.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = final_datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bcd7e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_id': 0, 'words': ['Glucose', '6-phosphate', 'dehydrogenase', 'variants', ':', 'Gd', '(', '+', ')', 'Alexandra', 'associated', 'with', 'neonatal', 'jaundice', 'and', 'Gd', '(', '-', ')', 'Camperdown', 'in', 'a', 'young', 'man', 'with', 'lamellar', 'cataracts', '.'], 'labels': [-100, 8, 8, -100, -100, 8, -100, -100, -100, -100, 8, 8, 8, -100, 8, 8, 8, 8, -100, -100, 8, 8, 3, -100, -100, 7, -100, -100, -100, 8, 8, -100, 8, 8, 8, 8, -100, -100, 8, 8, 8, 8, 8, 8, -100, 8, -100, -100, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], 'input_ids': [101, 20636, 127, 118, 19273, 1260, 7889, 23632, 19790, 6530, 10317, 131, 176, 1181, 113, 116, 114, 23280, 20192, 17394, 2628, 1114, 24762, 6575, 1233, 179, 3984, 12090, 2093, 1105, 176, 1181, 113, 118, 114, 3227, 25081, 13798, 1107, 170, 1685, 1299, 1114, 25492, 12576, 5855, 4626, 15585, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(str(tokenized_datasets['train'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4849200f-ca4c-4e31-83ab-66c842da82e8",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48f3d084",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(label_list) \n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33281cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer \n",
    "\n",
    "args = TrainingArguments( \n",
    "    \"test-ner\",\n",
    "    eval_strategy =\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "166fb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f35e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef327ef2-3520-455d-b104-719f4c8ca892",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59bed31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = final_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f774a8a-2192-42a6-a356-67054fd9baca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-CompositeMention',\n",
       " 'B-DiseaseClass',\n",
       " 'B-Modifier',\n",
       " 'B-SpecificDisease',\n",
       " 'I-CompositeMention',\n",
       " 'I-DiseaseClass',\n",
       " 'I-Modifier',\n",
       " 'I-SpecificDisease',\n",
       " 'O']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = final_datasets[\"train\"].features[\"labels\"].feature.names \n",
    "\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3528bed-47d9-4315-be33-33be579452c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "3\n",
      "7\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for i in example[\"labels\"]:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d4414f4-77d7-4555-a997-dbe891f06ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-SpecificDisease',\n",
       " 'I-SpecificDisease',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[\"labels\"]] \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fb97c73-4cef-4a92-9cb0-8fdad8f2a360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SpecificDisease': {'precision': np.float64(1.0),\n",
       "  'recall': np.float64(1.0),\n",
       "  'f1': np.float64(1.0),\n",
       "  'number': np.int64(1)},\n",
       " 'overall_precision': np.float64(1.0),\n",
       " 'overall_recall': np.float64(1.0),\n",
       " 'overall_f1': np.float64(1.0),\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=[labels], references=[labels]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc44bd-3094-4933-8cdc-20165cd5368b",
   "metadata": {},
   "source": [
    "### Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28722ba1-00f8-434a-84c5-3f77b339b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = final_datasets[\"train\"].features[\"labels\"].feature.names\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56886c-1fb7-4094-bb74-8afb9725891b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc2eef4f-b9f9-4196-864d-15bd6862dcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2108450/2660706628.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer( \n",
    "   model, \n",
    "   args, \n",
    "   train_dataset=tokenized_datasets[\"train\"], \n",
    "   eval_dataset=tokenized_datasets[\"validation\"], \n",
    "   data_collator=data_collator, \n",
    "   tokenizer=tokenizer, \n",
    "   compute_metrics=compute_metrics \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49748c83-1540-481a-b150-576ca852f257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2872' max='2872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2872/2872 07:30, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.152100</td>\n",
       "      <td>0.133177</td>\n",
       "      <td>0.451777</td>\n",
       "      <td>0.465359</td>\n",
       "      <td>0.458467</td>\n",
       "      <td>0.960367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.088941</td>\n",
       "      <td>0.647990</td>\n",
       "      <td>0.695425</td>\n",
       "      <td>0.670870</td>\n",
       "      <td>0.976176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.077540</td>\n",
       "      <td>0.721166</td>\n",
       "      <td>0.743791</td>\n",
       "      <td>0.732304</td>\n",
       "      <td>0.979497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.074903</td>\n",
       "      <td>0.717365</td>\n",
       "      <td>0.783007</td>\n",
       "      <td>0.748750</td>\n",
       "      <td>0.980028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.073153</td>\n",
       "      <td>0.718343</td>\n",
       "      <td>0.793464</td>\n",
       "      <td>0.754037</td>\n",
       "      <td>0.979807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.074668</td>\n",
       "      <td>0.741050</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.774797</td>\n",
       "      <td>0.980693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.074617</td>\n",
       "      <td>0.755152</td>\n",
       "      <td>0.814379</td>\n",
       "      <td>0.783648</td>\n",
       "      <td>0.981357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.075496</td>\n",
       "      <td>0.749395</td>\n",
       "      <td>0.809150</td>\n",
       "      <td>0.778127</td>\n",
       "      <td>0.981401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcanodeb/miniconda3/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2872, training_loss=0.1270265408213092, metrics={'train_runtime': 451.2898, 'train_samples_per_second': 101.558, 'train_steps_per_second': 6.364, 'total_flos': 2994127714768896.0, 'train_loss': 0.1270265408213092, 'epoch': 8.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3876e48f-0082-44f8-a465-854bd2cdd8b8",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02b96ce3-2b7a-4331-b590-10e4407d318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4366df8-830b-4a6d-bfc7-cc0e5a2cd6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/vocab.txt',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d69f826-1b69-43e6-8896-b9db24aacdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    str(i): label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: str(i) for i,label in enumerate(label_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "672f51b1-1a9b-403f-8a49-1fc5ece3a7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'B-CompositeMention',\n",
       " '1': 'B-DiseaseClass',\n",
       " '2': 'B-Modifier',\n",
       " '3': 'B-SpecificDisease',\n",
       " '4': 'I-CompositeMention',\n",
       " '5': 'I-DiseaseClass',\n",
       " '6': 'I-Modifier',\n",
       " '7': 'I-SpecificDisease',\n",
       " '8': 'O'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "288bc1c9-34e5-4967-8681-882a9a73f15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-CompositeMention': '0',\n",
       " 'B-DiseaseClass': '1',\n",
       " 'B-Modifier': '2',\n",
       " 'B-SpecificDisease': '3',\n",
       " 'I-CompositeMention': '4',\n",
       " 'I-DiseaseClass': '5',\n",
       " 'I-Modifier': '6',\n",
       " 'I-SpecificDisease': '7',\n",
       " 'O': '8'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9493db-0817-4991-8791-5f4e004e625c",
   "metadata": {},
   "source": [
    "## Loading model and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d71b6463-a750-409c-b7b0-5ebddbe059e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "603493d7-39fe-4965-9bef-3817b802bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"ner_model/config.json\"))\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6951ad1-128a-4fe2-8b84-f26ab7417ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b041b97b-2dda-4d62-bb55-6b14ea824f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████| 978/978 [00:00<00:00, 4683.57 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62/62 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08291115611791611, 'eval_precision': 0.7431102362204725, 'eval_recall': 0.8040468583599574, 'eval_f1': 0.7723785166240409, 'eval_accuracy': 0.9782458264855981, 'eval_runtime': 3.101, 'eval_samples_per_second': 315.381, 'eval_steps_per_second': 19.993, 'epoch': 8.0}\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"ner_model\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer\")\n",
    "tokenized_test = final_datasets[\"test\"].map(tokenize_and_align_labels, batched=True)\n",
    "results = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cdf2504-7d3d-4fdc-921f-a93e0b936015",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv(\"evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab017f8a-290f-494f-b349-43f3b9348c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
