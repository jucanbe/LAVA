{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3990913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (4.50.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (0.21.1)\n",
      "Requirement already satisfied: seqeval in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from seqeval) (1.5.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jcanodeb\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pandas numpy transformers accelerate datasets tokenizers seqeval evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad36e4-2065-4ab3-a522-4eef0530cb98",
   "metadata": {},
   "source": [
    "## Token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca459c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datasets \n",
    "import numpy as np \n",
    "from transformers import BertTokenizerFast \n",
    "from transformers import DataCollatorForTokenClassification \n",
    "from transformers import AutoModelForTokenClassification \n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "780f833b-1f41-4d28-b1b8-4b02688db304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': 'Late-onset', 'sentence_id': 0, 'labels': 'O'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(dtype='string', id=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"./datasets/preprocessed_NER/biored/train.csv\", \"validation\": \"./datasets/preprocessed_NER/biored/dev.csv\", \"test\": \"./datasets/preprocessed_NER/biored/test.csv\"}\n",
    "raw_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "print(raw_dataset[\"train\"][0])\n",
    "raw_dataset[\"train\"].features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49242446-1d70-42d2-9d90-e54106770480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6579e90cf3a4c88b22385364e5c5e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/4342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964fe5091bee4de982ffc303008a7571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ee9cce2a95447ba675448ba5997a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_id': 0, 'words': ['Late-onset', 'metachromatic', 'leukodystrophy', ':', 'molecular', 'pathology', 'in', 'two', 'siblings', '.'], 'labels': [12, 2, 8, 12, 12, 12, 12, 12, 12, 12]}\n",
      "Lista de etiquetas: ['B-CellLine', 'B-ChemicalEntity', 'B-DiseaseOrPhenotypicFeature', 'B-GeneOrGeneProduct', 'B-OrganismTaxon', 'B-SequenceVariant', 'I-CellLine', 'I-ChemicalEntity', 'I-DiseaseOrPhenotypicFeature', 'I-GeneOrGeneProduct', 'I-OrganismTaxon', 'I-SequenceVariant', 'O']\n"
     ]
    }
   ],
   "source": [
    "grouped_datasets = {}\n",
    "\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    dataset = raw_dataset[split]\n",
    "    grouped = defaultdict(lambda: {\"words\": [], \"labels\": []})\n",
    "\n",
    "    for example in dataset:\n",
    "        sid = example[\"sentence_id\"]\n",
    "        grouped[sid][\"words\"].append(example[\"words\"])\n",
    "        grouped[sid][\"labels\"].append(example[\"labels\"])\n",
    "\n",
    "    grouped_list = []\n",
    "    for sid, data in grouped.items():\n",
    "        grouped_list.append({\n",
    "            \"sentence_id\": sid,\n",
    "            \"words\": data[\"words\"],\n",
    "            \"labels\": data[\"labels\"]\n",
    "        })\n",
    "    grouped_datasets[split] = Dataset.from_list(grouped_list)\n",
    "\n",
    "all_labels = set()\n",
    "for example in grouped_datasets[\"train\"]:\n",
    "    all_labels.update(example[\"labels\"])\n",
    "label_list = sorted(list(all_labels))\n",
    "\n",
    "label_feature = ClassLabel(names=label_list)\n",
    "\n",
    "features = Features({\n",
    "    \"sentence_id\": Value(\"int32\"),\n",
    "    \"words\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(label_feature),\n",
    "})\n",
    "\n",
    "final_datasets = DatasetDict()\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    grouped_datasets[split] = grouped_datasets[split].cast(features)\n",
    "\n",
    "final_datasets = DatasetDict(grouped_datasets)\n",
    "\n",
    "print(final_datasets[\"train\"][0])\n",
    "print(\"List of labels:\", final_datasets[\"train\"].features[\"labels\"].feature.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "895fa0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (4342, 3), 'validation': (1127, 3), 'test': (1096, 3)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de4616e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_id': 0,\n",
       " 'words': ['Late-onset',\n",
       "  'metachromatic',\n",
       "  'leukodystrophy',\n",
       "  ':',\n",
       "  'molecular',\n",
       "  'pathology',\n",
       "  'in',\n",
       "  'two',\n",
       "  'siblings',\n",
       "  '.'],\n",
       " 'labels': [12, 2, 8, 12, 12, 12, 12, 12, 12, 12]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "925566eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['B-CellLine', 'B-ChemicalEntity', 'B-DiseaseOrPhenotypicFeature', 'B-GeneOrGeneProduct', 'B-OrganismTaxon', 'B-SequenceVariant', 'I-CellLine', 'I-ChemicalEntity', 'I-DiseaseOrPhenotypicFeature', 'I-GeneOrGeneProduct', 'I-OrganismTaxon', 'I-SequenceVariant', 'O'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets[\"train\"].features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "82ca23fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d488085eab4224911a8b45bf5f5b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcanodeb\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jcanodeb\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070d1e28674e4f9b94efe1d0e1202889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44acf1f25e4462281ca73bdf7e100b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b03506aee946ee8ed8121dcf014106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3318b88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, None]\n"
     ]
    }
   ],
   "source": [
    "example_text = final_datasets['train'][0]\n",
    "tokenized_input = tokenizer(example_text[\"words\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "word_ids = tokenized_input.word_ids()\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "079ccd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'late',\n",
       " '-',\n",
       " 'onset',\n",
       " 'meta',\n",
       " '##ch',\n",
       " '##romatic',\n",
       " 'le',\n",
       " '##uk',\n",
       " '##od',\n",
       " '##yst',\n",
       " '##rop',\n",
       " '##hy',\n",
       " ':',\n",
       " 'molecular',\n",
       " 'pathology',\n",
       " 'in',\n",
       " 'two',\n",
       " 'siblings',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f0cc280c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 21)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_text['labels']), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2567c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    texts = [[str(token) for token in sent] for sent in examples[\"words\"]]\n",
    "    tokenized_inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    labels_all = []\n",
    "    for i, word_labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(word_labels[word_idx])\n",
    "            else:\n",
    "                # We decide whether sub-tokens inherit the label or are ignored.\n",
    "                # Ignore -> -100 ; if not -> word_labels[word_idx].\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels_all.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels_all\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6c84ac8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_id': [4],\n",
       " 'words': [['A',\n",
       "   'comparison',\n",
       "   'of',\n",
       "   'genotypes',\n",
       "   ',',\n",
       "   'ARSA',\n",
       "   'activities',\n",
       "   ',',\n",
       "   'and',\n",
       "   'clinical',\n",
       "   'data',\n",
       "   'on',\n",
       "   '4',\n",
       "   'individuals',\n",
       "   'carrying',\n",
       "   'the',\n",
       "   'allele',\n",
       "   'of',\n",
       "   '81',\n",
       "   'patients',\n",
       "   'with',\n",
       "   'MLD',\n",
       "   'examined',\n",
       "   ',',\n",
       "   'further',\n",
       "   'validates',\n",
       "   'the',\n",
       "   'concept',\n",
       "   'that',\n",
       "   'different',\n",
       "   'degrees',\n",
       "   'of',\n",
       "   'residual',\n",
       "   'ARSA',\n",
       "   'activity',\n",
       "   'are',\n",
       "   'the',\n",
       "   'basis',\n",
       "   'of',\n",
       "   'phenotypical',\n",
       "   'variation',\n",
       "   'in',\n",
       "   'MLD',\n",
       "   '..']],\n",
       " 'labels': [[12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   3,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   4,\n",
       "   12,\n",
       "   2,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   3,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   2,\n",
       "   12]]}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets['train'][4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2e0e19e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1037, 7831, 1997, 8991, 26305, 2015, 1010, 29393, 2050, 3450, 1010, 1998, 6612, 2951, 2006, 1018, 3633, 4755, 1996, 2035, 12260, 1997, 6282, 5022, 2007, 19875, 2094, 8920, 1010, 2582, 9398, 8520, 1996, 4145, 2008, 2367, 5445, 1997, 21961, 29393, 2050, 4023, 2024, 1996, 3978, 1997, 6887, 16515, 27086, 8386, 1999, 19875, 2094, 1012, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 12, 12, 12, 12, -100, -100, 12, 3, -100, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100, 12, 12, 4, 12, 2, -100, 12, 12, 12, 12, -100, 12, 12, 12, 12, 12, 12, 12, 3, -100, 12, 12, 12, 12, 12, 12, -100, -100, 12, 12, 2, -100, 12, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]]}\n"
     ]
    }
   ],
   "source": [
    "q = tokenize_and_align_labels(final_datasets['train'][4:5]) \n",
    "print(q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cbfd5ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "a_______________________________________ 12\n",
      "comparison______________________________ 12\n",
      "of______________________________________ 12\n",
      "gen_____________________________________ 12\n",
      "##otype_________________________________ -100\n",
      "##s_____________________________________ -100\n",
      ",_______________________________________ 12\n",
      "ars_____________________________________ 3\n",
      "##a_____________________________________ -100\n",
      "activities______________________________ 12\n",
      ",_______________________________________ 12\n",
      "and_____________________________________ 12\n",
      "clinical________________________________ 12\n",
      "data____________________________________ 12\n",
      "on______________________________________ 12\n",
      "4_______________________________________ 12\n",
      "individuals_____________________________ 12\n",
      "carrying________________________________ 12\n",
      "the_____________________________________ 12\n",
      "all_____________________________________ 12\n",
      "##ele___________________________________ -100\n",
      "of______________________________________ 12\n",
      "81______________________________________ 12\n",
      "patients________________________________ 4\n",
      "with____________________________________ 12\n",
      "ml______________________________________ 2\n",
      "##d_____________________________________ -100\n",
      "examined________________________________ 12\n",
      ",_______________________________________ 12\n",
      "further_________________________________ 12\n",
      "valid___________________________________ 12\n",
      "##ates__________________________________ -100\n",
      "the_____________________________________ 12\n",
      "concept_________________________________ 12\n",
      "that____________________________________ 12\n",
      "different_______________________________ 12\n",
      "degrees_________________________________ 12\n",
      "of______________________________________ 12\n",
      "residual________________________________ 12\n",
      "ars_____________________________________ 3\n",
      "##a_____________________________________ -100\n",
      "activity________________________________ 12\n",
      "are_____________________________________ 12\n",
      "the_____________________________________ 12\n",
      "basis___________________________________ 12\n",
      "of______________________________________ 12\n",
      "ph______________________________________ 12\n",
      "##eno___________________________________ -100\n",
      "##typical_______________________________ -100\n",
      "variation_______________________________ 12\n",
      "in______________________________________ 12\n",
      "ml______________________________________ 2\n",
      "##d_____________________________________ -100\n",
      "._______________________________________ 12\n",
      "._______________________________________ -100\n",
      "[SEP]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n",
      "[PAD]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]): \n",
    "    print(f\"{token:_<40} {label}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f54b24a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2aa4da49174183bb6da6625dfcd559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c706bf1297483ca577adbdb42a5100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd612a24cab4299abc13ec90e9c80ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = final_datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1bcd7e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_id': 0, 'words': ['Late-onset', 'metachromatic', 'leukodystrophy', ':', 'molecular', 'pathology', 'in', 'two', 'siblings', '.'], 'labels': [-100, 12, -100, -100, 2, -100, -100, 8, -100, -100, -100, -100, -100, 12, 12, 12, 12, 12, 12, 12, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], 'input_ids': [101, 2397, 1011, 14447, 18804, 2818, 23645, 3393, 6968, 7716, 27268, 18981, 10536, 1024, 8382, 19314, 1999, 2048, 9504, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(str(tokenized_datasets['train'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4849200f-ca4c-4e31-83ab-66c842da82e8",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "48f3d084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(label_list) \n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "33281cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer \n",
    "\n",
    "args = TrainingArguments( \n",
    "    \"test-ner\",\n",
    "    eval_strategy = \"epoch\", \n",
    "    learning_rate=2e-5, \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16, \n",
    "    num_train_epochs=3, \n",
    "    weight_decay=0.01, \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "166fb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6f35e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef327ef2-3520-455d-b104-719f4c8ca892",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "59bed31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = final_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1f774a8a-2192-42a6-a356-67054fd9baca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-CellLine',\n",
       " 'B-ChemicalEntity',\n",
       " 'B-DiseaseOrPhenotypicFeature',\n",
       " 'B-GeneOrGeneProduct',\n",
       " 'B-OrganismTaxon',\n",
       " 'B-SequenceVariant',\n",
       " 'I-CellLine',\n",
       " 'I-ChemicalEntity',\n",
       " 'I-DiseaseOrPhenotypicFeature',\n",
       " 'I-GeneOrGeneProduct',\n",
       " 'I-OrganismTaxon',\n",
       " 'I-SequenceVariant',\n",
       " 'O']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = final_datasets[\"train\"].features[\"labels\"].feature.names \n",
    "\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e3528bed-47d9-4315-be33-33be579452c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "2\n",
      "8\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "for i in example[\"labels\"]:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1d4414f4-77d7-4555-a997-dbe891f06ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-DiseaseOrPhenotypicFeature',\n",
       " 'I-DiseaseOrPhenotypicFeature',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[\"labels\"]] \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4fb97c73-4cef-4a92-9cb0-8fdad8f2a360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DiseaseOrPhenotypicFeature': {'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1': 1.0,\n",
       "  'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=[labels], references=[labels]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc44bd-3094-4933-8cdc-20165cd5368b",
   "metadata": {},
   "source": [
    "### Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "28722ba1-00f8-434a-84c5-3f77b339b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, labels = eval_preds \n",
    "    pred_ids = np.argmax(pred_logits, axis=2) \n",
    "\n",
    "    predictions = [ \n",
    "        [label_list[p] for (p, l) in zip(pred, label) if l != -100] \n",
    "        for pred, label in zip(pred_ids, labels)\n",
    "    ] \n",
    "\n",
    "    true_labels = [ \n",
    "        [label_list[l] for (p, l) in zip(pred, label) if l != -100] \n",
    "        for pred, label in zip(pred_ids, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=predictions, references=true_labels)\n",
    "\n",
    "    return { \n",
    "          \"precision\": results[\"overall_precision\"], \n",
    "          \"recall\": results[\"overall_recall\"], \n",
    "          \"f1\": results[\"overall_f1\"], \n",
    "          \"accuracy\": results[\"overall_accuracy\"], \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56886c-1fb7-4094-bb74-8afb9725891b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "dc2eef4f-b9f9-4196-864d-15bd6862dcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcanodeb\\AppData\\Local\\Temp\\ipykernel_19624\\2660706628.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer( \n",
    "   model, \n",
    "   args, \n",
    "   train_dataset=tokenized_datasets[\"train\"], \n",
    "   eval_dataset=tokenized_datasets[\"validation\"], \n",
    "   data_collator=data_collator, \n",
    "   tokenizer=tokenizer, \n",
    "   compute_metrics=compute_metrics \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "49748c83-1540-481a-b150-576ca852f257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='816' max='816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [816/816 1:17:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.172772</td>\n",
       "      <td>0.761509</td>\n",
       "      <td>0.807885</td>\n",
       "      <td>0.784011</td>\n",
       "      <td>0.949325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.239900</td>\n",
       "      <td>0.161970</td>\n",
       "      <td>0.783987</td>\n",
       "      <td>0.829656</td>\n",
       "      <td>0.806175</td>\n",
       "      <td>0.955184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.239900</td>\n",
       "      <td>0.159738</td>\n",
       "      <td>0.787762</td>\n",
       "      <td>0.840836</td>\n",
       "      <td>0.813434</td>\n",
       "      <td>0.956520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcanodeb\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jcanodeb\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jcanodeb\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=816, training_loss=0.17959202504625507, metrics={'train_runtime': 4628.2381, 'train_samples_per_second': 2.814, 'train_steps_per_second': 0.176, 'total_flos': 850997511876096.0, 'train_loss': 0.17959202504625507, 'epoch': 3.0})"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3876e48f-0082-44f8-a465-854bd2cdd8b8",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "02b96ce3-2b7a-4331-b590-10e4407d318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b4366df8-830b-4a6d-bfc7-cc0e5a2cd6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\vocab.txt',\n",
       " 'tokenizer\\\\added_tokens.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7d69f826-1b69-43e6-8896-b9db24aacdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    str(i): label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: str(i) for i,label in enumerate(label_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "672f51b1-1a9b-403f-8a49-1fc5ece3a7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'B-CellLine',\n",
       " '1': 'B-ChemicalEntity',\n",
       " '2': 'B-DiseaseOrPhenotypicFeature',\n",
       " '3': 'B-GeneOrGeneProduct',\n",
       " '4': 'B-OrganismTaxon',\n",
       " '5': 'B-SequenceVariant',\n",
       " '6': 'I-CellLine',\n",
       " '7': 'I-ChemicalEntity',\n",
       " '8': 'I-DiseaseOrPhenotypicFeature',\n",
       " '9': 'I-GeneOrGeneProduct',\n",
       " '10': 'I-OrganismTaxon',\n",
       " '11': 'I-SequenceVariant',\n",
       " '12': 'O'}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "288bc1c9-34e5-4967-8681-882a9a73f15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-CellLine': '0',\n",
       " 'B-ChemicalEntity': '1',\n",
       " 'B-DiseaseOrPhenotypicFeature': '2',\n",
       " 'B-GeneOrGeneProduct': '3',\n",
       " 'B-OrganismTaxon': '4',\n",
       " 'B-SequenceVariant': '5',\n",
       " 'I-CellLine': '6',\n",
       " 'I-ChemicalEntity': '7',\n",
       " 'I-DiseaseOrPhenotypicFeature': '8',\n",
       " 'I-GeneOrGeneProduct': '9',\n",
       " 'I-OrganismTaxon': '10',\n",
       " 'I-SequenceVariant': '11',\n",
       " 'O': '12'}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9493db-0817-4991-8791-5f4e004e625c",
   "metadata": {},
   "source": [
    "## Loading model and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d71b6463-a750-409c-b7b0-5ebddbe059e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "603493d7-39fe-4965-9bef-3817b802bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"ner_model/config.json\"))\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c6951ad1-128a-4fe2-8b84-f26ab7417ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b041b97b-2dda-4d62-bb55-6b14ea824f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a4cb4a7ecc4e998d425c01e24815f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 01:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13821668922901154, 'eval_precision': 0.7817133443163097, 'eval_recall': 0.8341634925285673, 'eval_f1': 0.8070871722182849, 'eval_accuracy': 0.9584826656257762, 'eval_runtime': 107.5048, 'eval_samples_per_second': 10.195, 'eval_steps_per_second': 0.642, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"ner_model\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"tokenizer\")\n",
    "tokenized_test = final_datasets[\"test\"].map(tokenize_and_align_labels, batched=True)\n",
    "results = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf2504-7d3d-4fdc-921f-a93e0b936015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
