{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3990913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.2.4)\n",
      "Requirement already satisfied: transformers in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (4.51.2)\n",
      "Requirement already satisfied: accelerate in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: datasets in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: tokenizers in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (0.21.1)\n",
      "Requirement already satisfied: seqeval in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: evaluate in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pandas numpy transformers accelerate datasets tokenizers seqeval evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad36e4-2065-4ab3-a522-4eef0530cb98",
   "metadata": {},
   "source": [
    "## Token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca459c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcanodeb/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d555d5-2565-4fb6-9837-3722bf1a3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_REGISTRY = {\n",
    "    \"bert\": \"google-bert/bert-base-uncased\",\n",
    "    \"bert-large\": \"google-bert/bert-large-uncased\",\n",
    "    \"biobert\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
    "    \"bluebert\": \"bionlp/bluebert_pubmed_uncased_L-24_H-1024_A-16\",\n",
    "    \"clinical-bert\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    \"biomed_roberta\": \"allenai/biomed_roberta_base\",\n",
    "    \"pubmedbert\": \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "}\n",
    "\n",
    "\n",
    "model_to_use = \"ner_model/new_model\"\n",
    "tokenizer_path = \"ner_model/tokenizer\"\n",
    "dataset_to_use = \"combined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15ae1ee-c937-43b7-99dd-3e4383f2bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def filter_classes_from_dataset(dataset_dict, min_examples=25):\n",
    "    label_counts = Counter(dataset_dict[\"train\"][\"relation\"])\n",
    "    allowed_classes = {label for label, count in label_counts.items() if count >= min_examples}\n",
    "    def is_valid(example):\n",
    "        return example[\"relation\"] in allowed_classes\n",
    "\n",
    "    filtered_dataset = dataset_dict.filter(is_valid)\n",
    "    unique_labels = sorted(list(allowed_classes))\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    \n",
    "    def map_label(example):\n",
    "        example[\"label\"] = label2id[example[\"relation\"]]\n",
    "        return example\n",
    "        \n",
    "    filtered_dataset = filtered_dataset.map(map_label)\n",
    "    return filtered_dataset, label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f688dd-7226-4d0d-be90-2dfc3a32a508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<e1>Hepatocyte nuclear factor-6</e1>: associations between genetic variability and <e2>type II diabetes</e2> and between genetic variability and estimates of insulin secretion.', 'relation': 'Association', 'source': 'biored'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": f\"datasets/preprocessed_RE/{dataset_to_use}/{dataset_to_use}_train.csv\",\n",
    "    \"validation\": f\"datasets/preprocessed_RE/{dataset_to_use}/{dataset_to_use}_dev.csv\",\n",
    "    \"test\": f\"datasets/preprocessed_RE/{dataset_to_use}/{dataset_to_use}_test.csv\"\n",
    "}\n",
    "raw_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\",\")\n",
    "\n",
    "print(raw_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf18fa5-bf4e-4ff0-ac97-96eb83382612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association          → 7283\n",
      "Positive_Correlation → 4007\n",
      "Negative_Correlation → 3013\n",
      "Downregulator        → 2251\n",
      "Regulator            → 1652\n",
      "Upregulator          → 774\n",
      "Substrate            → 727\n",
      "Part_of              → 307\n",
      "Not                  → 240\n",
      "Antagonist           → 229\n",
      "Bind                 → 217\n",
      "Agonist              → 173\n",
      "Comparison           → 171\n",
      "Cotreatment          → 154\n",
      "Drug_Interaction     → 54\n",
      "Cofactor             → 34\n",
      "Modulator            → 29\n",
      "Conversion           → 3\n",
      "Undefined            → 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "relation_counts = Counter(raw_dataset[\"train\"][\"relation\"])\n",
    "\n",
    "for relation, count in relation_counts.most_common():\n",
    "    print(f\"{relation:20} → {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad86e9ea-e2be-4008-a59b-851357c61a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|█████████████████████████████████████████████████████| 21319/21319 [00:00<00:00, 432369.65 examples/s]\n",
      "Filter: 100%|███████████████████████████████████████████████████████| 8284/8284 [00:00<00:00, 408786.36 examples/s]\n",
      "Filter: 100%|███████████████████████████████████████████████████████| 9515/9515 [00:00<00:00, 419995.40 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████| 21198/21198 [00:00<00:00, 29062.06 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████| 8261/8261 [00:00<00:00, 34615.64 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████| 9442/9442 [00:00<00:00, 34858.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "raw_dataset, label2id, id2label = filter_classes_from_dataset(raw_dataset, min_examples=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643a7e63-6388-4956-a65f-ebc2c9b7d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'relation', 'source', 'label'],\n",
      "        num_rows: 21198\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'relation', 'source', 'label'],\n",
      "        num_rows: 8261\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'relation', 'source', 'label'],\n",
      "        num_rows: 9442\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e649bc1-cc11-4f0c-9f03-409c3ac28468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████| 21198/21198 [00:00<00:00, 51973.92 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████| 8261/8261 [00:00<00:00, 48459.67 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████| 9442/9442 [00:00<00:00, 51103.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "unique_labels = list(set(raw_dataset[\"train\"][\"relation\"]))\n",
    "label2id = {label: i for i, label in enumerate(sorted(unique_labels))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "def encode_labels(example):\n",
    "    example[\"label\"] = label2id[example[\"relation\"]]\n",
    "    return example\n",
    "\n",
    "raw_dataset = raw_dataset.map(encode_labels)\n",
    "\n",
    "train_labels = raw_dataset[\"train\"][\"label\"]\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f810c15e-b8f1-4e06-aaa0-2dcd2919071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████| 21198/21198 [00:01<00:00, 17420.69 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████| 8261/8261 [00:00<00:00, 14670.08 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████| 9442/9442 [00:00<00:00, 19273.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "def tokenize_example(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize_example, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe103548-9a15-438b-a9c9-84b612533b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7087879e-5920-4199-af40-852b7c39c508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ner_model/new_model and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ner_model/new_model and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([13]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([13, 1024]) in the checkpoint and torch.Size([14, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_to_use,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9db5d4-6592-4cbf-a668-fc8ddeb1fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc8db470-ea39-402b-86fc-2281aaaed2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro', zero_division=0)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd7d7c5d-0327-4ad3-92d3-cd7d4af62a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2246180/3493705534.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d124e445-239d-470d-b10e-bb0df53efdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "train_batch_size = training_args.per_device_train_batch_size\n",
    "train_dataset_size = len(tokenized_dataset[\"train\"])\n",
    "gradient_accumulation = training_args.gradient_accumulation_steps\n",
    "epochs = training_args.num_train_epochs\n",
    "\n",
    "total_steps = (train_dataset_size // (train_batch_size * gradient_accumulation)) * epochs\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    trainer.model.parameters(),\n",
    "    lr=training_args.learning_rate,\n",
    "    weight_decay=training_args.weight_decay\n",
    ")\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "trainer.optimizer = optimizer\n",
    "trainer.lr_scheduler = scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc5237cb-7ab9-46de-96f2-16a5e38f3ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6625' max='6625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6625/6625 07:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.361000</td>\n",
       "      <td>1.474852</td>\n",
       "      <td>0.531050</td>\n",
       "      <td>0.452839</td>\n",
       "      <td>0.508118</td>\n",
       "      <td>0.430853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.826300</td>\n",
       "      <td>1.295058</td>\n",
       "      <td>0.625953</td>\n",
       "      <td>0.541039</td>\n",
       "      <td>0.592242</td>\n",
       "      <td>0.550196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>1.419270</td>\n",
       "      <td>0.623169</td>\n",
       "      <td>0.593897</td>\n",
       "      <td>0.608342</td>\n",
       "      <td>0.581833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>1.535952</td>\n",
       "      <td>0.638906</td>\n",
       "      <td>0.588572</td>\n",
       "      <td>0.629814</td>\n",
       "      <td>0.590050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.357700</td>\n",
       "      <td>1.717069</td>\n",
       "      <td>0.633458</td>\n",
       "      <td>0.558779</td>\n",
       "      <td>0.622119</td>\n",
       "      <td>0.572448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.293800</td>\n",
       "      <td>1.767856</td>\n",
       "      <td>0.660211</td>\n",
       "      <td>0.638648</td>\n",
       "      <td>0.631362</td>\n",
       "      <td>0.613194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>1.803636</td>\n",
       "      <td>0.654521</td>\n",
       "      <td>0.618453</td>\n",
       "      <td>0.634846</td>\n",
       "      <td>0.617383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>1.803435</td>\n",
       "      <td>0.674010</td>\n",
       "      <td>0.620006</td>\n",
       "      <td>0.640935</td>\n",
       "      <td>0.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>1.885846</td>\n",
       "      <td>0.678731</td>\n",
       "      <td>0.633225</td>\n",
       "      <td>0.630050</td>\n",
       "      <td>0.615150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.237600</td>\n",
       "      <td>1.979587</td>\n",
       "      <td>0.664447</td>\n",
       "      <td>0.654285</td>\n",
       "      <td>0.627582</td>\n",
       "      <td>0.610436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>1.980561</td>\n",
       "      <td>0.672921</td>\n",
       "      <td>0.636450</td>\n",
       "      <td>0.626702</td>\n",
       "      <td>0.610883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>1.988767</td>\n",
       "      <td>0.674737</td>\n",
       "      <td>0.627280</td>\n",
       "      <td>0.630301</td>\n",
       "      <td>0.614792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>2.005168</td>\n",
       "      <td>0.671589</td>\n",
       "      <td>0.623410</td>\n",
       "      <td>0.628843</td>\n",
       "      <td>0.612535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6625, training_loss=0.5045039020034502, metrics={'train_runtime': 458.2361, 'train_samples_per_second': 231.3, 'train_steps_per_second': 14.458, 'total_flos': 2.469485252371968e+16, 'train_loss': 0.5045039020034502, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adc5bea8-66cb-470d-b89e-978584449443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./RE_model/tokenizer/tokenizer_config.json',\n",
       " './RE_model/tokenizer/special_tokens_map.json',\n",
       " './RE_model/tokenizer/vocab.txt',\n",
       " './RE_model/tokenizer/added_tokens.json',\n",
       " './RE_model/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./RE_model/model\"\n",
    "tokenizer_path = \"./RE_model/tokenizer\"\n",
    "\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef327ef2-3520-455d-b104-719f4c8ca892",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9493db-0817-4991-8791-5f4e004e625c",
   "metadata": {},
   "source": [
    "## Loading model and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f41b274-8da0-4281-bad4-2e4eaffe7ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config = json.load(open(\"RE_model/model/config.json\"))\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"RE_model/model/config.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "539bbdcf-12a6-4ca7-9c04-befe5b88cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cb3d37f-2b08-4858-b3a0-441c8bc7ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████| 9442/9442 [00:00<00:00, 16399.22 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.638489842414856, 'eval_accuracy': 0.6725270069900445, 'eval_precision': 0.6885509555521117, 'eval_recall': 0.677392440515314, 'eval_f1': 0.6689876118928787, 'eval_runtime': 7.6069, 'eval_samples_per_second': 1241.236, 'eval_steps_per_second': 77.692, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"RE_model/model/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"RE_model/tokenizer\")\n",
    "tokenized_test = raw_dataset[\"test\"].map(tokenize_example, batched=True)\n",
    "results = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60362faa-abec-4235-85ae-b1c9161a83a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label: {0: 'Agonist', 1: 'Antagonist', 2: 'Association', 3: 'Bind', 4: 'Comparison', 5: 'Cotreatment', 6: 'Downregulator', 7: 'Negative_Correlation', 8: 'Not', 9: 'Part_of', 10: 'Positive_Correlation', 11: 'Regulator', 12: 'Substrate', 13: 'Upregulator'}\n",
      "Unique labels in dataset: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13)]\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Agonist       0.57      0.78      0.66       193\n",
      "          Antagonist       0.76      0.80      0.78       293\n",
      "         Association       0.75      0.67      0.71      1796\n",
      "                Bind       0.50      0.71      0.59        17\n",
      "          Comparison       0.71      0.81      0.76        37\n",
      "         Cotreatment       0.95      0.41      0.57        44\n",
      "       Downregulator       0.75      0.78      0.76      1666\n",
      "Negative_Correlation       0.52      0.53      0.53       659\n",
      "                 Not       0.72      0.48      0.57       266\n",
      "             Part_of       0.91      0.84      0.87       207\n",
      "Positive_Correlation       0.59      0.64      0.61      1218\n",
      "           Regulator       0.71      0.59      0.64      1737\n",
      "           Substrate       0.73      0.80      0.77       643\n",
      "         Upregulator       0.47      0.66      0.55       666\n",
      "\n",
      "            accuracy                           0.67      9442\n",
      "           macro avg       0.69      0.68      0.67      9442\n",
      "        weighted avg       0.68      0.67      0.67      9442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "id2label = model.config.id2label\n",
    "\n",
    "present_labels = sorted(set(labels) | set(preds))\n",
    "target_names = [id2label[i] for i in present_labels]\n",
    "\n",
    "print(\"id2label:\", id2label)\n",
    "print(\"Unique labels in dataset:\", sorted(set(labels)))\n",
    "\n",
    "print(classification_report(\n",
    "    labels,\n",
    "    preds,\n",
    "    labels=present_labels,\n",
    "    target_names=target_names,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3b11c-d400-4a97-b922-14ee10955b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
