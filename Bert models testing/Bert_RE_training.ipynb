{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3990913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: pandas in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (2.2.4)\n",
      "Requirement already satisfied: transformers in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (4.51.2)\n",
      "Requirement already satisfied: accelerate in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: datasets in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: tokenizers in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (0.21.1)\n",
      "Requirement already satisfied: seqeval in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: evaluate in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jcanodeb/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pandas numpy transformers accelerate datasets tokenizers seqeval evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad36e4-2065-4ab3-a522-4eef0530cb98",
   "metadata": {},
   "source": [
    "## Token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca459c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcanodeb/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d555d5-2565-4fb6-9837-3722bf1a3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_REGISTRY = {\n",
    "    \"bert\": \"google-bert/bert-base-uncased\",\n",
    "    \"bert-large\": \"google-bert/bert-large-uncased\",\n",
    "    \"biobert\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
    "    \"bluebert\": \"bionlp/bluebert_pubmed_uncased_L-24_H-1024_A-16\",\n",
    "    \"clinical-bert\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    \"biomed_roberta\": \"allenai/biomed_roberta_base\",\n",
    "    \"pubmedbert\": \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "}\n",
    "\n",
    "\n",
    "model_to_use = \"google-bert/bert-large-uncased\"\n",
    "base_path = Path(\"./datasets/preprocessed_RE/biored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dcfd492-dffc-4c8b-9aa2-019c994b80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_rows = []\n",
    "all_test_rows = []\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace @XXXXXX$ with <eN>XXXXXX</eN> according to the order of appearance in the text.\n",
    "    \"\"\"\n",
    "    matches = list(re.finditer(r'@([A-Z]+)\\$', text))\n",
    "    new_text = text\n",
    "    offset = 0\n",
    "\n",
    "    for idx, match in enumerate(matches):\n",
    "        entity_type = match.group(1)\n",
    "        tag = f\"<e{idx+1}>{entity_type}</e{idx+1}>\"\n",
    "        start, end = match.span()\n",
    "        new_text = new_text[:start + offset] + tag + new_text[end + offset:]\n",
    "        offset += len(tag) - (end - start)\n",
    "    return new_text\n",
    "\n",
    "for i in range(1, 11):\n",
    "    subfolder = base_path / str(i)\n",
    "    train_file = subfolder / \"train.tsv\"\n",
    "    test_file = subfolder / \"test.tsv\"\n",
    "\n",
    "    if train_file.exists():\n",
    "        df = pd.read_csv(train_file, sep=\"\\t\", header=None)\n",
    "        if df.shape[1] > 2:\n",
    "            df = df.iloc[:, -2:]\n",
    "        df.iloc[:, 0] = df.iloc[:, 0].apply(preprocess_text)\n",
    "        all_train_rows.append(df)\n",
    "\n",
    "    if test_file.exists():\n",
    "        with open(test_file) as f:\n",
    "            has_header = \"index\" in f.readline()\n",
    "        df = pd.read_csv(test_file, sep=\"\\t\", header=0 if has_header else None)\n",
    "        if df.shape[1] == 3:\n",
    "            df = df.iloc[:, 1:]\n",
    "        df.iloc[:, 0] = df.iloc[:, 0].apply(preprocess_text)\n",
    "        all_test_rows.append(df)\n",
    "\n",
    "if all_train_rows:\n",
    "    train_accumulated = pd.concat(all_train_rows, ignore_index=True)\n",
    "    train_accumulated.to_csv(base_path / \"train_accumulated.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "else:\n",
    "    print(\"train.tsv not found\")\n",
    "\n",
    "if all_test_rows:\n",
    "    test_accumulated = pd.concat(all_test_rows, ignore_index=True)\n",
    "    test_accumulated.to_csv(base_path / \"test_accumulated.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "else:\n",
    "    print(\"test.tsv not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f688dd-7226-4d0d-be90-2dfc3a32a508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3195 examples [00:00, 463679.50 examples/s]\n",
      "Generating test split: 355 examples [00:00, 176064.55 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'In humans, loss-of-function of the cilia-centrosomal protein <e1>GENE</e1> is associated with Joubert and <e2>DISEASE</e2>, whereas hypomorphic mutations result in Leber congenital amaurosis (LCA), a form of early-onset retinal dystrophy.', 'label': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\": str(base_path/\"train_accumulated.tsv\"),\n",
    "    \"test\": str(base_path/\"test_accumulated.tsv\")\n",
    "}\n",
    "\n",
    "raw_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_files,\n",
    "    delimiter=\"\\t\",\n",
    "    column_names=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "print(raw_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f810c15e-b8f1-4e06-aaa0-2dcd2919071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 3195/3195 [00:00<00:00, 18514.33 examples/s]\n",
      "Map: 100%|██████████████████████████| 355/355 [00:00<00:00, 18159.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "def tokenize_example(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize_example, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe103548-9a15-438b-a9c9-84b612533b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7087879e-5920-4199-af40-852b7c39c508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_to_use,\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e9db5d4-6592-4cbf-a668-fc8ddeb1fe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1054363/386926744.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8db470-ea39-402b-86fc-2281aaaed2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d7c5d-0327-4ad3-92d3-cd7d4af62a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc5237cb-7ab9-46de-96f2-16a5e38f3ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 01:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.508400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.170900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.037200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.19304501334826152, metrics={'train_runtime': 80.5001, 'train_samples_per_second': 119.068, 'train_steps_per_second': 7.453, 'total_flos': 2233140529374720.0, 'train_loss': 0.19304501334826152, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adc5bea8-66cb-470d-b89e-978584449443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./RE_model/tokenizer/tokenizer_config.json',\n",
       " './RE_model/tokenizer/special_tokens_map.json',\n",
       " './RE_model/tokenizer/vocab.txt',\n",
       " './RE_model/tokenizer/added_tokens.json',\n",
       " './RE_model/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./RE_model/model\"\n",
    "tokenizer_path = \"./RE_model/tokenizer\"\n",
    "\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a73b9efb-f7fb-4e55-9eda-8e62dcb6fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"./ER_model/model\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"./ER_model/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef327ef2-3520-455d-b104-719f4c8ca892",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9493db-0817-4991-8791-5f4e004e625c",
   "metadata": {},
   "source": [
    "## Loading model and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c8c654c-2d00-4d47-9ce6-45787ef82938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 355/355 [00:00<00:00, 17566.99 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over test_accumulated.tsv\n",
      "Accuracy : 0.992\n",
      "Precision: 1.000\n",
      "Recall   : 0.989\n",
      "F1 Score : 0.994\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"./RE_model/model\").eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./RE_model/tokenizer\")\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "df = pd.read_csv(\"./datasets/preprocessed_RE/biored/test_accumulated.tsv\", sep=\"\\t\", header=None, names=[\"text\", \"label\"])\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "tokenized = dataset.map(lambda e: tokenizer(e[\"text\"], padding=\"max_length\", truncation=True, max_length=128), batched=True)\n",
    "tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "preds, labels = [], []\n",
    "for batch in torch.utils.data.DataLoader(tokenized, batch_size=16):\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        labels.extend(batch[\"label\"].numpy())\n",
    "\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "\n",
    "print(\"Evaluation over test_accumulated.tsv\")\n",
    "print(f\"Accuracy : {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall   : {recall:.3f}\")\n",
    "print(f\"F1 Score : {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60362faa-abec-4235-85ae-b1c9161a83a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
